import enum
import logging
from dataclasses import dataclass, field
from typing import Any, List, Union, Optional, Set

from dataclasses_json import dataclass_json, config
from dagshub.data_engine.dtypes import MetadataFieldType, ReservedTags

logger = logging.getLogger(__name__)


@dataclass
class Metadata:
    key: str
    value: Any


autogenerated_columns = {
    "path",
    "datapoint_id",
    "dagshub_download_url",
}


class IntegrationStatus(enum.Enum):
    VALID = "VALID"
    INVALID_CREDENTIALS = "INVALID_CREDENTIALS"
    MISSING = "MISSING"


class PreprocessingStatus(enum.Enum):
    READY = "READY"
    IN_PROGRESS = "IN_PROGRESS"
    AUTO_SCAN_IN_PROGRESS = "AUTO_SCAN_IN_PROGRESS"
    FAILED = "FAILED"


class DatasourceType(enum.Enum):
    BUCKET = "BUCKET"
    REPOSITORY = "REPOSITORY"
    CUSTOM = "CUSTOM"


class ScanOption(str, enum.Enum):
    """
    Enum of options that can be applied during scanning process with
    :func:`~dagshub.data_engine.model.datasource.Datasource.scan_source`
    """

    FORCE_REGENERATE_AUTO_SCAN_VALUES = "FORCE_REGENERATE_AUTO_SCAN_VALUES"
    """
    Regenerate all the autogenerated metadata values for the whole datasource
    """


@dataclass_json
@dataclass
class MetadataFieldSchema:
    # This should match the GraphQL schema: MetadataFieldProps
    name: str
    valueType: MetadataFieldType = field(metadata=config(encoder=lambda val: val.value))
    multiple: bool
    tags: Optional[Set[str]]

    def __repr__(self):
        res = f"{self.name} ({self.valueType.value})"
        if self.tags is not None and len(self.tags) > 0:
            res += f" with tags: {self.tags}"
        return res

    def is_annotation(self):
        return ReservedTags.ANNOTATION.value in self.tags if self.tags else False


@dataclass
class DatasourceResult:
    id: Union[str, int]
    name: str
    rootUrl: str
    integrationStatus: IntegrationStatus
    preprocessingStatus: PreprocessingStatus
    type: DatasourceType
    metadataFields: Optional[List[MetadataFieldSchema]]


@dataclass
class DatasetResult:
    id: Union[str, int]
    name: str
    datasource: DatasourceResult
    datasetQuery: str


@dataclass_json
@dataclass
class DatasourceSerializedState:
    """
    Serialized state of the datasource.
    This should be enough to recreate the exact copy of the datasource back (with some requests maybe)

    Also carries additional information about which dataset it had assigned,
    if the state of the datasource at the point of saving differed from the dataset, the timestamp of saving
    """

    repo: str
    """Repository this datasource is on"""
    datasource_id: Union[str, int]
    """ID of the datasource"""
    datasource_name: str
    """Name of the datasource"""
    query: Optional[dict] = None
    """Query at the time of saving"""
    dataset_id: Optional[Union[str, int]] = None
    """ID of the assigned dataset"""
    dataset_name: Optional[str] = None
    """Name of the assigned dataset"""
    timestamp: Optional[float] = None
    """Timestamp of serialization"""
    modified: Optional[bool] = None
    """Does the query differ from the query in the assigned dataset"""
    link: Optional[str] = None
    """URL to open this datasource on DagsHub"""

    @property
    def has_query(self):
        return (self.query is not None) and (self.query.get("query") is not None)
